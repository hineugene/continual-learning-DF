{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model \n",
    "from Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ewc\n",
    "from ewc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=1)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=1)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_empirical(model, OPTIMIZER, MAX_LABEL, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "    weights = model.trainable_weights\n",
    "    fisher_matrix = [tf.zeros_like(w) for w in weights]\n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) < MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = to_input(train, MAX_LABEL)\n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=1)\n",
    "\n",
    "                test_seq, test_label = to_input(test, MAX_LABEL)\n",
    "                loss, accuracy = model.evaluate(test_seq, test_label, batch_size=32, verbose=1)\n",
    "                print(f\"Task_0 training accuracy: {accuracy:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix_empirical(model, train_seq, train_label, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "                \n",
    "                train_seq, train_label = to_input(train, MAX_LABEL)\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=1)\n",
    "\n",
    "\n",
    "                inc_test_seq, inc_test_label = to_input(inc_test, MAX_LABEL)\n",
    "                loss, inc_accuracy = model.evaluate(inc_test_seq, inc_test_label, batch_size=32, verbose=1)\n",
    "\n",
    "                print(f\"Task_{i} accuracy: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "                test_seq, test_label = to_input(test, MAX_LABEL)\n",
    "\n",
    "                loss, accuracy = model.evaluate(test_seq, test_label, batch_size=32, verbose=1)\n",
    "                print(f\"Task ~{i-1} accuracy after training on Task_{i}: {accuracy:.4f}\")\n",
    "                \n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix_empirical(model, test_seq, test_label, num_sample=len(test_seq))\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_nonbase_AB(model, OPTIMIZER, data, test_size, task_A, task_B,\n",
    "                      MAX_LABEL, epochs_A, epochs_B):\n",
    "\n",
    "    model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    ###### task A\n",
    "    part_A = split_by_label(data, 0, task_A)\n",
    "    train_A, test_A = split_train_test(part_A, test_size=test_size, random_state=11)\n",
    "\n",
    "    train_A_seq, train_A_label = to_input(train_A, MAX_LABEL = MAX_LABEL)\n",
    "\n",
    "    history = model.fit(x=train_A_seq, y=train_A_label, epochs=epochs_A, verbose=2)\n",
    "\n",
    "    test_A_seq, test_A_label = to_input(test_A, MAX_LABEL = MAX_LABEL)\n",
    "    loss, accuracy = model.evaluate(test_A_seq, test_A_label, batch_size=32, verbose=1)\n",
    "\n",
    "    print(f\"\\ntask A Test Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    ###### task B\n",
    "    part_B = split_by_label(data, task_A+1, task_A+task_B)\n",
    "    train_B, test_B = split_train_test(part_B, test_size=test_size, random_state=11)\n",
    "\n",
    "    train_B_seq, train_B_label = to_input(train_B, MAX_LABEL = MAX_LABEL)\n",
    "\n",
    "    history = model.fit(x=train_B_seq, y=train_B_label, epochs=epochs_B, verbose=2)\n",
    "\n",
    "    test_B_seq, test_B_label = to_input(test_B, MAX_LABEL = MAX_LABEL)\n",
    "    loss, accuracy = model.evaluate(test_B_seq, test_B_label, batch_size=32, verbose=1)\n",
    "\n",
    "    print(f\"\\ntask B Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_A_seq, test_A_label, batch_size=32, verbose=1)\n",
    "    print(f\"\\nforgetted Accuracy: {accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naivis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
