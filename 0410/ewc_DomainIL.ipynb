{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model_DomainIL\n",
    "from Model_DomainIL import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_set):\n",
    "  acc = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "  for i, (seq, labels) in enumerate(test_set):\n",
    "    preds = model.predict_on_batch(seq)\n",
    "    acc.update_state(labels, preds)\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb):   \n",
    "    loss = 0\n",
    "    current = model.trainable_weights \n",
    "    \n",
    "    for F, c, o in zip(fisher_matrix, current, optimal_weights):\n",
    "        loss += tf.reduce_sum(F * ((c - o) ** 2))\n",
    "\n",
    "\n",
    "    return loss * (lamb / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(model, fisher_matrix, lamb):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = BinaryCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb=lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix(model, data, num_sample=10):\n",
    "    \n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(tensor) for tensor in weights]\n",
    "\n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "\n",
    "    for i in indices:\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            output = model(x, training=False) \n",
    "            log_likelihood = tf.math.log(output)\n",
    "\n",
    "        gradients = tape.gradient(log_likelihood, weights)\n",
    "        variance = [var + (grad ** 2) for var, grad in zip(variance, gradients)]\n",
    "\n",
    "    fisher_matrix = [tensor / num_sample for tensor in variance]\n",
    "    \n",
    "    return fisher_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix_empirical(model, data, label, num_sample=100):\n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(w) for w in weights]\n",
    "\n",
    "\n",
    "    for i in indices:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            y = tf.expand_dims(label[i], axis=0) \n",
    "            logits = model(x, training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, logits, from_logits=False)\n",
    "\n",
    "        gradients = tape.gradient(loss, weights)\n",
    "        for j in range(len(variance)):\n",
    "            if gradients[j] is not None:\n",
    "                variance[j] += tf.square(gradients[j])\n",
    "\n",
    "    fisher_matrix = [v / num_sample for v in variance]\n",
    "\n",
    "    # [디버깅 추가]\n",
    "    print(\"\\nlogits : [DEBUG] Fisher matrix shapes:\")\n",
    "    for i, f in enumerate(fisher_matrix):\n",
    "        print(f\" - Fisher {i}: {f.shape}, mean={tf.reduce_mean(f):.4f}, std={tf.math.reduce_std(f):.4f}\")\n",
    "\n",
    "    return fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix_logits(model, data, label, num_sample=100):\n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(w) for w in weights]\n",
    "\n",
    "\n",
    "    for i in indices:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            y = tf.expand_dims(label[i], axis=0) \n",
    "            logits = model(x, training=False)\n",
    "            class_idx = tf.argmax(logits, axis=1)\n",
    "            selected_logit = tf.gather_nd(logits, tf.stack([tf.range(tf.shape(class_idx)[0]), class_idx], axis=1))\n",
    "            # loss = tf.keras.losses.categorical_crossentropy(y, logits, from_logits=False)\n",
    "\n",
    "        gradients = tape.gradient(selected_logit, weights)\n",
    "        for j in range(len(variance)):\n",
    "            if gradients[j] is not None:\n",
    "                variance[j] += tf.square(gradients[j])\n",
    "\n",
    "    fisher_matrix = [v / num_sample for v in variance]\n",
    "\n",
    "    \n",
    "    # [디버깅 추가]\n",
    "    print(\"\\nlogits : [DEBUG] Fisher matrix shapes:\")\n",
    "    for i, f in enumerate(fisher_matrix):\n",
    "        print(f\" - Fisher {i}: {f.shape}, mean={tf.reduce_mean(f):.4f}, std={tf.math.reduce_std(f):.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    return fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nAIvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
