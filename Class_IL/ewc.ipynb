{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model \n",
    "from Model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb):   \n",
    "    loss = 0\n",
    "    current = model.trainable_weights \n",
    "    \n",
    "    for F, c, o in zip(fisher_matrix, current, optimal_weights):\n",
    "        loss += tf.reduce_sum(F * ((c - o) ** 2))\n",
    "\n",
    "\n",
    "    return loss * (lamb / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(model, fisher_matrix, lamb):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = CategoricalCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb=lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix(model, data, num_sample=10):\n",
    "\n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(tensor) for tensor in weights]\n",
    "\n",
    "    # num_sample 개의 데이터 랜덤샘플링 \n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "\n",
    "    for i in indices:\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            output = model(x, training=False) # (수정) 메모리 문제, 모든 데이터를 한번에 넣으면 오류 생김. 여기서는 하나씩 열개의 데이터를 사용 \n",
    "            log_likelihood = tf.math.log(output)\n",
    "\n",
    "        gradients = tape.gradient(log_likelihood, weights)\n",
    "        variance = [var + (grad ** 2) for var, grad in zip(variance, gradients)]\n",
    "\n",
    "    fisher_matrix = [tensor / num_sample for tensor in variance]\n",
    "    \n",
    "    return fisher_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (수정) 배치를 사용하지 않음 \n",
    "def evaluate(model, test_set):\n",
    "  acc = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "  for i, (seq, labels) in enumerate(test_set):\n",
    "    preds = model.predict_on_batch(seq)\n",
    "    acc.update_state(labels, preds)\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_joint(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                inc_train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "                train = accumulate_data(train, inc_train)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ce ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss_ce(model, fisher_matrix, lamb, alpha):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = alpha * CategoricalCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb=lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop_ce(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, alpha=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=2)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb, alpha=alpha), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=2)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('mon_data.pkl')\n",
    "print(data.shape)\n",
    "MAX_LABEL = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adamax.py:99: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 13s - loss: 3.8934 - accuracy: 0.0708 - 13s/epoch - 67ms/step\n",
      "   First_task training accuracy: 0.0708\n",
      "250/250 - 18s - loss: 3.8948 - accuracy: 0.1074 - 18s/epoch - 73ms/step\n",
      "   1_task training accuracy: 0.1074\n",
      "Task 1 accuracy after training on Task ~0: 0.0219\n",
      "300/300 - 21s - loss: 3.8765 - accuracy: 0.1266 - 21s/epoch - 70ms/step\n",
      "   2_task training accuracy: 0.1266\n",
      "Task 2 accuracy after training on Task ~1: 0.0440\n",
      "350/350 - 24s - loss: 3.8438 - accuracy: 0.1442 - 24s/epoch - 68ms/step\n",
      "   3_task training accuracy: 0.1442\n",
      "Task 3 accuracy after training on Task ~2: 0.0737\n",
      "400/400 - 26s - loss: 3.7137 - accuracy: 0.1667 - 26s/epoch - 66ms/step\n",
      "   4_task training accuracy: 0.1667\n",
      "Task 4 accuracy after training on Task ~3: 0.2318\n"
     ]
    }
   ],
   "source": [
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop_joint(model, OPTIMIZER, data, test_size=0.2, first_task = 39, inc_task = 10, first_epochs = 1, inc_epochs = 1, lamb=0.0, num_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adamax.py:99: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 - 13s - loss: 3.8196 - accuracy: 0.0886 - 13s/epoch - 64ms/step\n",
      "Epoch 2/100\n",
      "200/200 - 11s - loss: 3.2640 - accuracy: 0.1450 - 11s/epoch - 55ms/step\n",
      "Epoch 3/100\n",
      "200/200 - 11s - loss: 3.0052 - accuracy: 0.1955 - 11s/epoch - 56ms/step\n",
      "Epoch 4/100\n",
      "200/200 - 11s - loss: 2.8152 - accuracy: 0.2294 - 11s/epoch - 56ms/step\n",
      "Epoch 5/100\n",
      "200/200 - 11s - loss: 2.6011 - accuracy: 0.2800 - 11s/epoch - 57ms/step\n",
      "Epoch 6/100\n",
      "200/200 - 12s - loss: 2.4323 - accuracy: 0.3244 - 12s/epoch - 58ms/step\n",
      "Epoch 7/100\n",
      "200/200 - 12s - loss: 2.3025 - accuracy: 0.3527 - 12s/epoch - 58ms/step\n",
      "Epoch 8/100\n",
      "200/200 - 12s - loss: 2.1510 - accuracy: 0.3998 - 12s/epoch - 58ms/step\n",
      "Epoch 9/100\n",
      "200/200 - 12s - loss: 2.0243 - accuracy: 0.4372 - 12s/epoch - 58ms/step\n",
      "Epoch 10/100\n",
      "200/200 - 12s - loss: 1.9064 - accuracy: 0.4661 - 12s/epoch - 58ms/step\n",
      "Epoch 11/100\n",
      "200/200 - 12s - loss: 1.7742 - accuracy: 0.5008 - 12s/epoch - 58ms/step\n",
      "Epoch 12/100\n",
      "200/200 - 12s - loss: 1.6462 - accuracy: 0.5411 - 12s/epoch - 58ms/step\n",
      "Epoch 13/100\n",
      "200/200 - 12s - loss: 1.5496 - accuracy: 0.5708 - 12s/epoch - 58ms/step\n",
      "Epoch 14/100\n",
      "200/200 - 12s - loss: 1.4340 - accuracy: 0.6006 - 12s/epoch - 58ms/step\n",
      "Epoch 15/100\n",
      "200/200 - 12s - loss: 1.3177 - accuracy: 0.6358 - 12s/epoch - 58ms/step\n",
      "Epoch 16/100\n",
      "200/200 - 12s - loss: 1.2152 - accuracy: 0.6692 - 12s/epoch - 58ms/step\n",
      "Epoch 17/100\n",
      "200/200 - 12s - loss: 1.1427 - accuracy: 0.6842 - 12s/epoch - 58ms/step\n",
      "Epoch 18/100\n",
      "200/200 - 12s - loss: 1.0489 - accuracy: 0.7153 - 12s/epoch - 59ms/step\n",
      "Epoch 19/100\n",
      "200/200 - 12s - loss: 0.9573 - accuracy: 0.7448 - 12s/epoch - 59ms/step\n",
      "Epoch 20/100\n",
      "200/200 - 12s - loss: 0.8848 - accuracy: 0.7650 - 12s/epoch - 58ms/step\n",
      "Epoch 21/100\n",
      "200/200 - 12s - loss: 0.7944 - accuracy: 0.7862 - 12s/epoch - 58ms/step\n",
      "Epoch 22/100\n",
      "200/200 - 12s - loss: 0.7603 - accuracy: 0.7950 - 12s/epoch - 59ms/step\n",
      "Epoch 23/100\n",
      "200/200 - 12s - loss: 0.6723 - accuracy: 0.8239 - 12s/epoch - 59ms/step\n",
      "Epoch 24/100\n",
      "200/200 - 12s - loss: 0.6485 - accuracy: 0.8259 - 12s/epoch - 58ms/step\n",
      "Epoch 25/100\n",
      "200/200 - 12s - loss: 0.5928 - accuracy: 0.8462 - 12s/epoch - 58ms/step\n",
      "Epoch 26/100\n",
      "200/200 - 12s - loss: 0.5612 - accuracy: 0.8495 - 12s/epoch - 58ms/step\n",
      "Epoch 27/100\n",
      "200/200 - 12s - loss: 0.5194 - accuracy: 0.8636 - 12s/epoch - 58ms/step\n",
      "Epoch 28/100\n",
      "200/200 - 12s - loss: 0.4917 - accuracy: 0.8708 - 12s/epoch - 59ms/step\n",
      "Epoch 29/100\n",
      "200/200 - 12s - loss: 0.4523 - accuracy: 0.8755 - 12s/epoch - 59ms/step\n",
      "Epoch 30/100\n",
      "200/200 - 12s - loss: 0.4308 - accuracy: 0.8898 - 12s/epoch - 60ms/step\n",
      "Epoch 31/100\n",
      "200/200 - 12s - loss: 0.4020 - accuracy: 0.8933 - 12s/epoch - 58ms/step\n",
      "Epoch 32/100\n",
      "200/200 - 12s - loss: 0.3840 - accuracy: 0.8989 - 12s/epoch - 58ms/step\n",
      "Epoch 33/100\n",
      "200/200 - 12s - loss: 0.3706 - accuracy: 0.9067 - 12s/epoch - 58ms/step\n",
      "Epoch 34/100\n",
      "200/200 - 12s - loss: 0.3658 - accuracy: 0.9033 - 12s/epoch - 58ms/step\n",
      "Epoch 35/100\n",
      "200/200 - 12s - loss: 0.3280 - accuracy: 0.9141 - 12s/epoch - 58ms/step\n",
      "Epoch 36/100\n",
      "200/200 - 12s - loss: 0.3189 - accuracy: 0.9158 - 12s/epoch - 58ms/step\n",
      "Epoch 37/100\n",
      "200/200 - 12s - loss: 0.3043 - accuracy: 0.9228 - 12s/epoch - 58ms/step\n",
      "Epoch 38/100\n",
      "200/200 - 12s - loss: 0.2872 - accuracy: 0.9236 - 12s/epoch - 58ms/step\n",
      "Epoch 39/100\n",
      "200/200 - 12s - loss: 0.2700 - accuracy: 0.9277 - 12s/epoch - 59ms/step\n",
      "Epoch 40/100\n",
      "200/200 - 12s - loss: 0.2651 - accuracy: 0.9311 - 12s/epoch - 59ms/step\n",
      "Epoch 41/100\n",
      "200/200 - 12s - loss: 0.2433 - accuracy: 0.9356 - 12s/epoch - 58ms/step\n",
      "Epoch 42/100\n",
      "200/200 - 12s - loss: 0.2417 - accuracy: 0.9347 - 12s/epoch - 58ms/step\n",
      "Epoch 43/100\n",
      "200/200 - 12s - loss: 0.2343 - accuracy: 0.9398 - 12s/epoch - 58ms/step\n",
      "Epoch 44/100\n",
      "200/200 - 12s - loss: 0.2238 - accuracy: 0.9408 - 12s/epoch - 59ms/step\n",
      "Epoch 45/100\n",
      "200/200 - 12s - loss: 0.2153 - accuracy: 0.9445 - 12s/epoch - 58ms/step\n",
      "Epoch 46/100\n",
      "200/200 - 12s - loss: 0.1943 - accuracy: 0.9494 - 12s/epoch - 58ms/step\n",
      "Epoch 47/100\n",
      "200/200 - 12s - loss: 0.1864 - accuracy: 0.9528 - 12s/epoch - 58ms/step\n",
      "Epoch 48/100\n",
      "200/200 - 12s - loss: 0.1939 - accuracy: 0.9520 - 12s/epoch - 58ms/step\n",
      "Epoch 49/100\n",
      "200/200 - 12s - loss: 0.1762 - accuracy: 0.9534 - 12s/epoch - 60ms/step\n",
      "Epoch 50/100\n",
      "200/200 - 12s - loss: 0.1682 - accuracy: 0.9555 - 12s/epoch - 60ms/step\n",
      "Epoch 51/100\n",
      "200/200 - 12s - loss: 0.1630 - accuracy: 0.9586 - 12s/epoch - 59ms/step\n",
      "Epoch 52/100\n",
      "200/200 - 12s - loss: 0.1643 - accuracy: 0.9553 - 12s/epoch - 58ms/step\n",
      "Epoch 53/100\n",
      "200/200 - 12s - loss: 0.1535 - accuracy: 0.9603 - 12s/epoch - 59ms/step\n",
      "Epoch 54/100\n",
      "200/200 - 12s - loss: 0.1471 - accuracy: 0.9619 - 12s/epoch - 58ms/step\n",
      "Epoch 55/100\n",
      "200/200 - 12s - loss: 0.1574 - accuracy: 0.9594 - 12s/epoch - 58ms/step\n",
      "Epoch 56/100\n",
      "200/200 - 12s - loss: 0.1385 - accuracy: 0.9625 - 12s/epoch - 58ms/step\n",
      "Epoch 57/100\n",
      "200/200 - 12s - loss: 0.1466 - accuracy: 0.9591 - 12s/epoch - 58ms/step\n",
      "Epoch 58/100\n",
      "200/200 - 12s - loss: 0.1278 - accuracy: 0.9666 - 12s/epoch - 58ms/step\n",
      "Epoch 59/100\n",
      "200/200 - 12s - loss: 0.1354 - accuracy: 0.9623 - 12s/epoch - 59ms/step\n",
      "Epoch 60/100\n",
      "200/200 - 12s - loss: 0.1196 - accuracy: 0.9680 - 12s/epoch - 59ms/step\n",
      "Epoch 61/100\n",
      "200/200 - 12s - loss: 0.1246 - accuracy: 0.9680 - 12s/epoch - 59ms/step\n",
      "Epoch 62/100\n",
      "200/200 - 12s - loss: 0.1132 - accuracy: 0.9702 - 12s/epoch - 59ms/step\n",
      "Epoch 63/100\n",
      "200/200 - 12s - loss: 0.1201 - accuracy: 0.9645 - 12s/epoch - 58ms/step\n",
      "Epoch 64/100\n",
      "200/200 - 12s - loss: 0.1215 - accuracy: 0.9684 - 12s/epoch - 59ms/step\n",
      "Epoch 65/100\n",
      "200/200 - 12s - loss: 0.1121 - accuracy: 0.9700 - 12s/epoch - 58ms/step\n",
      "Epoch 66/100\n",
      "200/200 - 12s - loss: 0.1133 - accuracy: 0.9697 - 12s/epoch - 58ms/step\n",
      "Epoch 67/100\n",
      "200/200 - 12s - loss: 0.1096 - accuracy: 0.9689 - 12s/epoch - 58ms/step\n",
      "Epoch 68/100\n",
      "200/200 - 12s - loss: 0.1025 - accuracy: 0.9734 - 12s/epoch - 58ms/step\n",
      "Epoch 69/100\n",
      "200/200 - 12s - loss: 0.0994 - accuracy: 0.9720 - 12s/epoch - 59ms/step\n",
      "Epoch 70/100\n",
      "200/200 - 12s - loss: 0.0966 - accuracy: 0.9753 - 12s/epoch - 60ms/step\n",
      "Epoch 71/100\n",
      "200/200 - 12s - loss: 0.0977 - accuracy: 0.9734 - 12s/epoch - 59ms/step\n",
      "Epoch 72/100\n",
      "200/200 - 12s - loss: 0.1082 - accuracy: 0.9709 - 12s/epoch - 59ms/step\n",
      "Epoch 73/100\n",
      "200/200 - 12s - loss: 0.0947 - accuracy: 0.9708 - 12s/epoch - 58ms/step\n",
      "Epoch 74/100\n",
      "200/200 - 12s - loss: 0.0879 - accuracy: 0.9753 - 12s/epoch - 59ms/step\n",
      "Epoch 75/100\n",
      "200/200 - 12s - loss: 0.0941 - accuracy: 0.9741 - 12s/epoch - 58ms/step\n",
      "Epoch 76/100\n",
      "200/200 - 12s - loss: 0.0842 - accuracy: 0.9772 - 12s/epoch - 58ms/step\n",
      "Epoch 77/100\n",
      "200/200 - 12s - loss: 0.0896 - accuracy: 0.9778 - 12s/epoch - 58ms/step\n",
      "Epoch 78/100\n",
      "200/200 - 12s - loss: 0.0792 - accuracy: 0.9783 - 12s/epoch - 58ms/step\n",
      "Epoch 79/100\n",
      "200/200 - 12s - loss: 0.0820 - accuracy: 0.9781 - 12s/epoch - 58ms/step\n",
      "Epoch 80/100\n",
      "200/200 - 12s - loss: 0.0781 - accuracy: 0.9800 - 12s/epoch - 59ms/step\n",
      "Epoch 81/100\n",
      "200/200 - 12s - loss: 0.0873 - accuracy: 0.9759 - 12s/epoch - 59ms/step\n",
      "Epoch 82/100\n",
      "200/200 - 12s - loss: 0.0784 - accuracy: 0.9786 - 12s/epoch - 58ms/step\n",
      "Epoch 83/100\n",
      "200/200 - 12s - loss: 0.0795 - accuracy: 0.9775 - 12s/epoch - 58ms/step\n",
      "Epoch 84/100\n",
      "200/200 - 12s - loss: 0.0693 - accuracy: 0.9803 - 12s/epoch - 58ms/step\n",
      "Epoch 85/100\n",
      "200/200 - 11s - loss: 0.0731 - accuracy: 0.9795 - 11s/epoch - 57ms/step\n",
      "Epoch 86/100\n",
      "200/200 - 11s - loss: 0.0688 - accuracy: 0.9805 - 11s/epoch - 57ms/step\n",
      "Epoch 87/100\n",
      "200/200 - 11s - loss: 0.0702 - accuracy: 0.9794 - 11s/epoch - 57ms/step\n",
      "Epoch 88/100\n",
      "200/200 - 11s - loss: 0.0656 - accuracy: 0.9830 - 11s/epoch - 57ms/step\n",
      "Epoch 89/100\n",
      "200/200 - 11s - loss: 0.0756 - accuracy: 0.9797 - 11s/epoch - 57ms/step\n",
      "Epoch 90/100\n",
      "200/200 - 12s - loss: 0.0701 - accuracy: 0.9802 - 12s/epoch - 58ms/step\n",
      "Epoch 91/100\n",
      "200/200 - 12s - loss: 0.0697 - accuracy: 0.9808 - 12s/epoch - 59ms/step\n",
      "Epoch 92/100\n",
      "200/200 - 12s - loss: 0.0560 - accuracy: 0.9845 - 12s/epoch - 58ms/step\n",
      "Epoch 93/100\n",
      "200/200 - 11s - loss: 0.0611 - accuracy: 0.9836 - 11s/epoch - 57ms/step\n",
      "Epoch 94/100\n",
      "200/200 - 11s - loss: 0.0643 - accuracy: 0.9828 - 11s/epoch - 57ms/step\n",
      "Epoch 95/100\n",
      "200/200 - 11s - loss: 0.0548 - accuracy: 0.9855 - 11s/epoch - 57ms/step\n",
      "Epoch 96/100\n",
      "200/200 - 11s - loss: 0.0609 - accuracy: 0.9836 - 11s/epoch - 57ms/step\n",
      "Epoch 97/100\n",
      "200/200 - 11s - loss: 0.0621 - accuracy: 0.9831 - 11s/epoch - 57ms/step\n",
      "Epoch 98/100\n",
      "200/200 - 11s - loss: 0.0592 - accuracy: 0.9836 - 11s/epoch - 57ms/step\n",
      "Epoch 99/100\n",
      "200/200 - 11s - loss: 0.0628 - accuracy: 0.9808 - 11s/epoch - 57ms/step\n",
      "Epoch 100/100\n",
      "200/200 - 11s - loss: 0.0512 - accuracy: 0.9859 - 11s/epoch - 57ms/step\n",
      "   First_task training accuracy: 0.9859\n",
      "Epoch 1/50\n",
      "50/50 - 6s - loss: 32.0688 - accuracy: 0.0000e+00 - 6s/epoch - 127ms/step\n",
      "Epoch 2/50\n",
      "50/50 - 3s - loss: 18.9853 - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 3/50\n",
      "50/50 - 3s - loss: 14.3202 - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 4/50\n",
      "50/50 - 3s - loss: 11.9282 - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 5/50\n",
      "50/50 - 3s - loss: 10.0316 - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 6/50\n",
      "50/50 - 3s - loss: 8.2145 - accuracy: 0.0031 - 3s/epoch - 60ms/step\n",
      "Epoch 7/50\n",
      "50/50 - 3s - loss: 6.4388 - accuracy: 0.0169 - 3s/epoch - 58ms/step\n",
      "Epoch 8/50\n",
      "50/50 - 3s - loss: 5.8866 - accuracy: 0.0406 - 3s/epoch - 58ms/step\n",
      "Epoch 9/50\n",
      "50/50 - 3s - loss: 5.0410 - accuracy: 0.0650 - 3s/epoch - 58ms/step\n",
      "Epoch 10/50\n",
      "50/50 - 3s - loss: 4.3528 - accuracy: 0.1150 - 3s/epoch - 58ms/step\n",
      "Epoch 11/50\n",
      "50/50 - 3s - loss: 3.8714 - accuracy: 0.1631 - 3s/epoch - 59ms/step\n",
      "Epoch 12/50\n",
      "50/50 - 3s - loss: 3.5617 - accuracy: 0.1988 - 3s/epoch - 58ms/step\n",
      "Epoch 13/50\n",
      "50/50 - 3s - loss: 3.0377 - accuracy: 0.2062 - 3s/epoch - 58ms/step\n",
      "Epoch 14/50\n",
      "50/50 - 3s - loss: 2.8666 - accuracy: 0.2150 - 3s/epoch - 59ms/step\n",
      "Epoch 15/50\n",
      "50/50 - 3s - loss: 2.8820 - accuracy: 0.2212 - 3s/epoch - 59ms/step\n",
      "Epoch 16/50\n",
      "50/50 - 3s - loss: 2.5481 - accuracy: 0.2675 - 3s/epoch - 60ms/step\n",
      "Epoch 17/50\n",
      "50/50 - 3s - loss: 2.3541 - accuracy: 0.2831 - 3s/epoch - 59ms/step\n",
      "Epoch 18/50\n",
      "50/50 - 3s - loss: 2.3135 - accuracy: 0.2850 - 3s/epoch - 58ms/step\n",
      "Epoch 19/50\n",
      "50/50 - 3s - loss: 2.0584 - accuracy: 0.3656 - 3s/epoch - 59ms/step\n",
      "Epoch 20/50\n",
      "50/50 - 3s - loss: 1.9680 - accuracy: 0.3938 - 3s/epoch - 59ms/step\n",
      "Epoch 21/50\n",
      "50/50 - 3s - loss: 1.7838 - accuracy: 0.4550 - 3s/epoch - 59ms/step\n",
      "Epoch 22/50\n",
      "50/50 - 3s - loss: 1.6394 - accuracy: 0.5069 - 3s/epoch - 59ms/step\n",
      "Epoch 23/50\n",
      "50/50 - 3s - loss: 1.5010 - accuracy: 0.5700 - 3s/epoch - 59ms/step\n",
      "Epoch 24/50\n",
      "50/50 - 3s - loss: 1.4137 - accuracy: 0.5994 - 3s/epoch - 59ms/step\n",
      "Epoch 25/50\n",
      "50/50 - 3s - loss: 1.2540 - accuracy: 0.6331 - 3s/epoch - 59ms/step\n",
      "Epoch 26/50\n",
      "50/50 - 3s - loss: 1.0354 - accuracy: 0.7081 - 3s/epoch - 59ms/step\n",
      "Epoch 27/50\n",
      "50/50 - 3s - loss: 1.0262 - accuracy: 0.7237 - 3s/epoch - 59ms/step\n",
      "Epoch 28/50\n",
      "50/50 - 3s - loss: 0.8803 - accuracy: 0.7719 - 3s/epoch - 59ms/step\n",
      "Epoch 29/50\n",
      "50/50 - 3s - loss: 0.7590 - accuracy: 0.7994 - 3s/epoch - 59ms/step\n",
      "Epoch 30/50\n",
      "50/50 - 3s - loss: 0.6361 - accuracy: 0.8319 - 3s/epoch - 59ms/step\n",
      "Epoch 31/50\n",
      "50/50 - 3s - loss: 0.5558 - accuracy: 0.8594 - 3s/epoch - 59ms/step\n",
      "Epoch 32/50\n",
      "50/50 - 3s - loss: 0.5112 - accuracy: 0.8669 - 3s/epoch - 58ms/step\n",
      "Epoch 33/50\n",
      "50/50 - 3s - loss: 0.4251 - accuracy: 0.8888 - 3s/epoch - 59ms/step\n",
      "Epoch 34/50\n",
      "50/50 - 3s - loss: 0.4138 - accuracy: 0.8975 - 3s/epoch - 59ms/step\n",
      "Epoch 35/50\n",
      "50/50 - 3s - loss: 0.4005 - accuracy: 0.8950 - 3s/epoch - 59ms/step\n",
      "Epoch 36/50\n",
      "50/50 - 3s - loss: 0.3296 - accuracy: 0.9175 - 3s/epoch - 59ms/step\n",
      "Epoch 37/50\n",
      "50/50 - 3s - loss: 0.3196 - accuracy: 0.9275 - 3s/epoch - 60ms/step\n",
      "Epoch 38/50\n",
      "50/50 - 3s - loss: 0.2897 - accuracy: 0.9344 - 3s/epoch - 61ms/step\n",
      "Epoch 39/50\n",
      "50/50 - 3s - loss: 0.2535 - accuracy: 0.9362 - 3s/epoch - 61ms/step\n",
      "Epoch 40/50\n",
      "50/50 - 3s - loss: 0.2165 - accuracy: 0.9463 - 3s/epoch - 61ms/step\n",
      "Epoch 41/50\n",
      "50/50 - 3s - loss: 0.1745 - accuracy: 0.9594 - 3s/epoch - 60ms/step\n",
      "Epoch 42/50\n",
      "50/50 - 3s - loss: 0.2165 - accuracy: 0.9438 - 3s/epoch - 60ms/step\n",
      "Epoch 43/50\n",
      "50/50 - 3s - loss: 0.1853 - accuracy: 0.9513 - 3s/epoch - 59ms/step\n",
      "Epoch 44/50\n",
      "50/50 - 3s - loss: 0.2080 - accuracy: 0.9544 - 3s/epoch - 61ms/step\n",
      "Epoch 45/50\n",
      "50/50 - 3s - loss: 0.1637 - accuracy: 0.9600 - 3s/epoch - 61ms/step\n",
      "Epoch 46/50\n",
      "50/50 - 3s - loss: 0.1618 - accuracy: 0.9581 - 3s/epoch - 61ms/step\n",
      "Epoch 47/50\n",
      "50/50 - 3s - loss: 0.1435 - accuracy: 0.9638 - 3s/epoch - 60ms/step\n",
      "Epoch 48/50\n",
      "50/50 - 3s - loss: 0.1336 - accuracy: 0.9650 - 3s/epoch - 59ms/step\n",
      "Epoch 49/50\n",
      "50/50 - 3s - loss: 0.1548 - accuracy: 0.9638 - 3s/epoch - 59ms/step\n",
      "Epoch 50/50\n",
      "50/50 - 3s - loss: 0.1316 - accuracy: 0.9613 - 3s/epoch - 59ms/step\n",
      "   1_task training accuracy: 0.9613\n",
      "Task 1 accuracy after training on Task ~0: 0.0006\n",
      "Epoch 1/50\n",
      "50/50 - 7s - loss: nan - accuracy: 0.0000e+00 - 7s/epoch - 135ms/step\n",
      "Epoch 2/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 3/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 4/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 5/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 6/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 7/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 8/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 9/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 10/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 11/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 12/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 13/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 14/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 15/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 16/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 17/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 18/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 19/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 20/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 21/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 22/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 23/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 24/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 25/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 26/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 27/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 28/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 29/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 30/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 31/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 32/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 33/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 34/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 35/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 36/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 37/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 38/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 61ms/step\n",
      "Epoch 39/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 40/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 41/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 42/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 43/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 44/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 45/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 46/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 47/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 48/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 49/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 50/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "   2_task training accuracy: 0.0000\n",
      "Task 2 accuracy after training on Task ~1: 0.0200\n",
      "Epoch 1/50\n",
      "50/50 - 6s - loss: nan - accuracy: 0.0000e+00 - 6s/epoch - 111ms/step\n",
      "Epoch 2/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 3/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 4/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 56ms/step\n",
      "Epoch 5/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 6/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 7/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 8/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 9/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 10/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 11/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 12/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 13/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 14/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 15/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 16/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 60ms/step\n",
      "Epoch 17/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 18/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 19/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 20/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 21/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 22/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 23/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 59ms/step\n",
      "Epoch 24/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 25/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 26/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 27/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 28/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 29/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 30/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 31/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 32/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 58ms/step\n",
      "Epoch 33/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 34/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 35/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 36/50\n",
      "50/50 - 3s - loss: nan - accuracy: 0.0000e+00 - 3s/epoch - 57ms/step\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4432\\1777157838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 옵티마이저 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mOPTIMIZER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m39\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4432\\985893784.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, OPTIMIZER, data, test_size, first_task, inc_task, first_epochs, inc_epochs, lamb, num_sample)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minc_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2497\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1863\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 빌드 \n",
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop(model, OPTIMIZER, data, test_size=0.2, first_task = 39, inc_task = 10, first_epochs = 100, inc_epochs = 50, lamb=0.0, num_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naivis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
