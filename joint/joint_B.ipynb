{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint.ipynb (EWC 기반 Joint 학습)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "def load_mon_data():\n",
    "    data_a = pd.read_pickle('mon_data_A.pkl')\n",
    "    data_b = pd.read_pickle('mon_data_B.pkl')\n",
    "    return data_a, data_b\n",
    "\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data_a, data_b = load_mon_data()\n",
    "\n",
    "# mon_data_A를 train, test로 분리\n",
    "first_part = split_by_label(data_a, 0, 49)\n",
    "train_A, test_A = split_train_test(first_part, test_size=0.2, random_state=11)\n",
    "\n",
    "# 3D ndarray 로 변환 \n",
    "train_seq_A, train_label_A = split_data_label(train_A)\n",
    "\n",
    "train_seq_A = np.stack(train_seq_A.values)\n",
    "train_seq_A = train_seq_A[..., np.newaxis]\n",
    "\n",
    "train_label_A = train_label_A.values\n",
    "train_label_A = to_categorical(train_label_A, num_classes=95)\n",
    "\n",
    "# mon_data_B를 train, test로 분리\n",
    "second_part = split_by_label(data_b, 50, 94)\n",
    "train_B, test_B = split_train_test(second_part, test_size=0.2, random_state=11)\n",
    "\n",
    "#   3D ndarray 로 변환 \n",
    "train_seq_B, train_label_B = split_data_label(train_B)\n",
    "\n",
    "train_seq_B = np.stack(train_seq_B.values)\n",
    "train_seq_B = train_seq_B[..., np.newaxis]\n",
    "\n",
    "train_label_B = train_label_B.values\n",
    "train_label_B = to_categorical(train_label_B, num_classes=95)\n",
    "\n",
    "# mon_data_A의 train과 mon_data_B의 train 합치기\n",
    "def accumulate(data1, data2):\n",
    "    return np.concatenate([data1, data2], axis=0)\n",
    "train_seq = accumulate(train_seq_A, train_seq_B)\n",
    "train_label = accumulate(train_label_A, train_label_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D ndarray 로 변환 \n",
    "test_seq_A, test_label_A = split_data_label(test_A)\n",
    "\n",
    "test_seq_A = np.stack(test_seq_A.values)\n",
    "test_seq_A = test_seq_A[..., np.newaxis]\n",
    "\n",
    "test_label_A = test_label_A.values\n",
    "test_label_A = to_categorical(test_label_A, num_classes=95)\n",
    "\n",
    "#   3D ndarray 로 변환 \n",
    "test_seq_B, test_label_B = split_data_label(test_B)\n",
    "\n",
    "test_seq_B = np.stack(test_seq_B.values)\n",
    "test_seq_B = test_seq_B[..., np.newaxis]\n",
    "\n",
    "test_label_B = test_label_B.values\n",
    "test_label_B = to_categorical(test_label_B, num_classes=95)\n",
    "\n",
    "# mon_data_A의 test와 mon_data_B의 test 합치기\n",
    "test_seq = accumulate(test_seq_A, test_seq_B)\n",
    "test_label = accumulate(test_label_A, test_label_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "475/475 - 38s - loss: 4.8452 - accuracy: 0.4161 - 38s/epoch - 80ms/step\n",
      "Epoch 2/30\n",
      "475/475 - 27s - loss: 3.1246 - accuracy: 0.4252 - 27s/epoch - 58ms/step\n",
      "Epoch 3/30\n",
      "475/475 - 27s - loss: 2.8070 - accuracy: 0.4470 - 27s/epoch - 57ms/step\n",
      "Epoch 4/30\n",
      "475/475 - 27s - loss: 2.5721 - accuracy: 0.4635 - 27s/epoch - 58ms/step\n",
      "Epoch 5/30\n",
      "475/475 - 28s - loss: 2.3291 - accuracy: 0.4911 - 28s/epoch - 58ms/step\n",
      "Epoch 6/30\n",
      "475/475 - 27s - loss: 2.0635 - accuracy: 0.5341 - 27s/epoch - 58ms/step\n",
      "Epoch 7/30\n",
      "475/475 - 27s - loss: 1.8226 - accuracy: 0.5808 - 27s/epoch - 58ms/step\n",
      "Epoch 8/30\n",
      "475/475 - 27s - loss: 1.5727 - accuracy: 0.6311 - 27s/epoch - 58ms/step\n",
      "Epoch 9/30\n",
      "475/475 - 28s - loss: 1.3874 - accuracy: 0.6679 - 28s/epoch - 58ms/step\n",
      "Epoch 10/30\n",
      "475/475 - 28s - loss: 1.2304 - accuracy: 0.7040 - 28s/epoch - 59ms/step\n",
      "Epoch 11/30\n",
      "475/475 - 28s - loss: 1.1016 - accuracy: 0.7343 - 28s/epoch - 58ms/step\n",
      "Epoch 12/30\n",
      "475/475 - 28s - loss: 0.9912 - accuracy: 0.7526 - 28s/epoch - 58ms/step\n",
      "Epoch 13/30\n",
      "475/475 - 28s - loss: 0.9092 - accuracy: 0.7768 - 28s/epoch - 58ms/step\n",
      "Epoch 14/30\n",
      "475/475 - 28s - loss: 0.8336 - accuracy: 0.7948 - 28s/epoch - 58ms/step\n",
      "Epoch 15/30\n",
      "475/475 - 28s - loss: 0.7777 - accuracy: 0.8111 - 28s/epoch - 58ms/step\n",
      "Epoch 16/30\n",
      "475/475 - 27s - loss: 0.7110 - accuracy: 0.8252 - 27s/epoch - 58ms/step\n",
      "Epoch 17/30\n",
      "475/475 - 27s - loss: 0.6717 - accuracy: 0.8348 - 27s/epoch - 58ms/step\n",
      "Epoch 18/30\n",
      "475/475 - 27s - loss: 0.6369 - accuracy: 0.8413 - 27s/epoch - 58ms/step\n",
      "Epoch 19/30\n",
      "475/475 - 28s - loss: 0.5892 - accuracy: 0.8541 - 28s/epoch - 58ms/step\n",
      "Epoch 20/30\n",
      "475/475 - 28s - loss: 0.5591 - accuracy: 0.8643 - 28s/epoch - 59ms/step\n",
      "Epoch 21/30\n",
      "475/475 - 27s - loss: 0.5341 - accuracy: 0.8672 - 27s/epoch - 58ms/step\n",
      "Epoch 22/30\n",
      "475/475 - 28s - loss: 0.5018 - accuracy: 0.8748 - 28s/epoch - 58ms/step\n",
      "Epoch 23/30\n",
      "475/475 - 27s - loss: 0.4806 - accuracy: 0.8826 - 27s/epoch - 58ms/step\n",
      "Epoch 24/30\n",
      "475/475 - 27s - loss: 0.4565 - accuracy: 0.8870 - 27s/epoch - 58ms/step\n",
      "Epoch 25/30\n",
      "475/475 - 28s - loss: 0.4341 - accuracy: 0.8909 - 28s/epoch - 58ms/step\n",
      "Epoch 26/30\n",
      "475/475 - 27s - loss: 0.4085 - accuracy: 0.8949 - 27s/epoch - 58ms/step\n",
      "Epoch 27/30\n",
      "475/475 - 27s - loss: 0.4065 - accuracy: 0.8955 - 27s/epoch - 58ms/step\n",
      "Epoch 28/30\n",
      "475/475 - 28s - loss: 0.3781 - accuracy: 0.9058 - 28s/epoch - 58ms/step\n",
      "Epoch 29/30\n",
      "475/475 - 27s - loss: 0.3559 - accuracy: 0.9115 - 27s/epoch - 58ms/step\n",
      "Epoch 30/30\n",
      "475/475 - 27s - loss: 0.3419 - accuracy: 0.9153 - 27s/epoch - 58ms/step\n",
      "[Joint EWC] Accuracy on A+B after training on B: 0.9147\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "num_sample = 1000\n",
    "epochs=30\n",
    "MAX_LABEL =  95\n",
    "\n",
    "\n",
    "# joint_A에서 학습한 모델 및 정보 불러오기\n",
    "model = load_model(\"joint_model.h5\")\n",
    "\n",
    "# mon_data_A의 train과 mon_data_B의 train으로 학습 수행\n",
    "#train_seq, train_label = to_input(data, MAX_LABEL)\n",
    "\n",
    "# EWC 손실로 모델 재컴파일\n",
    "OPTIMIZER = Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# 학습\n",
    "model.fit(train_seq, train_label,\n",
    "          epochs=epochs,\n",
    "          verbose=2)\n",
    "\n",
    "# 평가: mon_data test셋 전체\n",
    "loss, acc = model.evaluate(test_seq, test_label, verbose=0)\n",
    "print(f\"[Joint EWC] Accuracy on A+B after training on B: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nAIvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
