{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model \n",
    "from Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb):   \n",
    "    loss = 0\n",
    "    current = model.trainable_weights \n",
    "    \n",
    "    for F, c, o in zip(fisher_matrix, current, optimal_weights):\n",
    "        loss += tf.reduce_sum(F * ((c - o) ** 2))\n",
    "\n",
    "\n",
    "    return loss * (lamb / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewc_loss(model, fisher_matrix, lamb):\n",
    "    optimal_weights = deepcopy(model.trainable_weights)\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "\n",
    "        ce_loss = CategoricalCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "        ewc_loss = compute_ewc_penalty(model, fisher_matrix, optimal_weights, lamb)\n",
    "\n",
    "        return ce_loss + ewc_loss\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_matrix(model, data, num_sample=10):\n",
    "\n",
    "    weights = model.trainable_weights\n",
    "    variance = [tf.zeros_like(tensor) for tensor in weights]\n",
    "\n",
    "    # num_sample 개의 데이터 랜덤샘플링 \n",
    "    indices = np.random.choice(len(data), size=num_sample, replace=False)\n",
    "\n",
    "    for i in indices:\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(weights)\n",
    "            x = tf.expand_dims(data[i], axis=0)\n",
    "            output = model(x, training=False) # (수정) 메모리 문제, 모든 데이터를 한번에 넣으면 오류 생김. 여기서는 하나씩 열개의 데이터를 사용 \n",
    "            log_likelihood = tf.math.log(output)\n",
    "\n",
    "        gradients = tape.gradient(log_likelihood, weights)\n",
    "        variance = [var + (grad ** 2) for var, grad in zip(variance, gradients)]\n",
    "\n",
    "    fisher_matrix = [tensor / num_sample for tensor in variance]\n",
    "    \n",
    "    return fisher_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (수정) 배치를 사용하지 않음 \n",
    "def evaluate(model, test_set):\n",
    "  acc = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "  for i, (seq, labels) in enumerate(test_set):\n",
    "    preds = model.predict_on_batch(seq)\n",
    "    acc.update_state(labels, preds)\n",
    "  return acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, OPTIMIZER, data, test_size,\n",
    "                first_task = 44, inc_task = 5, first_epochs = 30, inc_epochs = 5,\n",
    "                  lamb=0, num_sample=10):\n",
    "    \n",
    "    first_part = split_by_label(data, 0, first_task)\n",
    "    train, test = split_train_test(first_part, test_size=test_size, random_state=11)\n",
    "    \n",
    "\n",
    "    # OPTIMIZER -> param\n",
    "    i = 0\n",
    "    while(1):\n",
    "\n",
    "        if ( first_task + i * inc_task ) <= MAX_LABEL:\n",
    "            \n",
    "            if i == 0:\n",
    "                model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "                # 3D ndarray 로 변환 \n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "                \n",
    "\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=first_epochs, verbose=1)\n",
    "                print(f\"   First_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "            else:\n",
    "                # 데이터 준비 \n",
    "                inc_part = split_by_label(data, first_task + (i-1) * inc_task + 1, first_task + i * inc_task )\n",
    "                train, inc_test = split_train_test(inc_part, test_size=test_size, random_state=11)\n",
    "\n",
    "                model.compile(loss=ewc_loss(model, fisher_matrix, lamb=lamb), optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "                \n",
    "                # 3D ndarray 로 변환 (이부분 함수로 바꾸기)\n",
    "                train_seq, train_label = split_data_label(train)\n",
    "\n",
    "                train_seq = np.stack(train_seq.values)\n",
    "                train_seq = train_seq[..., np.newaxis]\n",
    "\n",
    "                train_label = train_label.values\n",
    "                train_label = to_categorical(train_label, num_classes=MAX_LABEL)\n",
    "\n",
    "\n",
    "\n",
    "                # train\n",
    "                history = model.fit(x=train_seq, y=train_label, epochs=inc_epochs, verbose=1)\n",
    "                print(f\"   {i}_task training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "                # (수정) 일종의 전처리이므로 preprocessing 또는 utils에 함수 작성 \n",
    "                # 축적된 test로 정확도 측정 (중요, EWC 성능)\n",
    "                test_seq, test_label = split_data_label(test)\n",
    "                test_seq = np.stack(test_seq.values)\n",
    "                test_seq = test_seq[..., np.newaxis]\n",
    "\n",
    "                test_label = test_label.values\n",
    "                test_label = to_categorical(test_label, num_classes=MAX_LABEL)\n",
    "\n",
    "                test_ = tf.data.Dataset.from_tensor_slices((test_seq, test_label))\n",
    "                test_ = test_.batch(32) #(수정) 모델 자체 배치 존재? - 학습시 fit 디폴트값도 32\n",
    "\n",
    "                inc_accuracy = evaluate(model, test_)\n",
    "                print(f\"Task {i} accuracy after training on Task ~{i-1}: {inc_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # test 업데이트 \n",
    "                test = accumulate_data(test, inc_test)\n",
    "\n",
    "                # Fisher matrix 계산 \n",
    "                fisher_matrix = compute_fisher_matrix(model, train_seq, num_sample=num_sample)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('mon_data.pkl')\n",
    "print(data.shape)\n",
    "MAX_LABEL = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adamax.py:99: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 58ms/step - loss: 3.6932 - accuracy: 0.0691\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 3.3335 - accuracy: 0.1091\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 3.1769 - accuracy: 0.1262\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 3.0134 - accuracy: 0.1547\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.8939 - accuracy: 0.1650\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.7992 - accuracy: 0.1944\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.7054 - accuracy: 0.2150\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.6357 - accuracy: 0.2250\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.5806 - accuracy: 0.2428\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.4939 - accuracy: 0.2512\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.4431 - accuracy: 0.2741\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.3895 - accuracy: 0.2884\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.3639 - accuracy: 0.2919\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.2822 - accuracy: 0.3044\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.2041 - accuracy: 0.3328\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.1679 - accuracy: 0.3481\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.1462 - accuracy: 0.3625\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.0960 - accuracy: 0.3634\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.0249 - accuracy: 0.3884\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.9934 - accuracy: 0.3894\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.9344 - accuracy: 0.3988\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.8764 - accuracy: 0.4322\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.8332 - accuracy: 0.4409\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.7663 - accuracy: 0.4578\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.7198 - accuracy: 0.4666\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 1.6675 - accuracy: 0.4881\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 1.6324 - accuracy: 0.5022\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 1.5732 - accuracy: 0.5169\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 1.5488 - accuracy: 0.5325\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 1.4891 - accuracy: 0.5384\n",
      "   First_task training accuracy: 0.5384\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 5s 69ms/step - loss: 9.7688 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 2s 64ms/step - loss: 8.0126 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 6.5891 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 2s 61ms/step - loss: 5.3930 - accuracy: 0.0050\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 4.5129 - accuracy: 0.0437\n",
      "   1_task training accuracy: 0.0437\n",
      "Task 1 accuracy after training on Task ~0: 0.0675\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 4s 63ms/step - loss: 8.5301 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 7.2690 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 6.4138 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 5.7460 - accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 5.1516 - accuracy: 0.0088\n",
      "   2_task training accuracy: 0.0088\n",
      "Task 2 accuracy after training on Task ~1: 0.0460\n"
     ]
    }
   ],
   "source": [
    "# 모델 빌드 \n",
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop(model, OPTIMIZER, data, test_size=0.2, first_task = 19, inc_task = 5, first_epochs = 30, inc_epochs = 5, lamb=0, num_sample=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\naivis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adamax.py:99: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 61ms/step - loss: 3.7213 - accuracy: 0.0716\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 3.3375 - accuracy: 0.1078\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 3.0899 - accuracy: 0.1453\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.9301 - accuracy: 0.1725\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.8551 - accuracy: 0.1794\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.7756 - accuracy: 0.1931\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.6635 - accuracy: 0.2262\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.6293 - accuracy: 0.2194\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.5537 - accuracy: 0.2491\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.4874 - accuracy: 0.2606\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.4368 - accuracy: 0.2756\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.3866 - accuracy: 0.2834\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 2.3242 - accuracy: 0.3047\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 2.3059 - accuracy: 0.3013\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.2188 - accuracy: 0.3288\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 2.1842 - accuracy: 0.3288\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.1576 - accuracy: 0.3434\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.0815 - accuracy: 0.3681\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.0504 - accuracy: 0.3819\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.0077 - accuracy: 0.3919\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.9519 - accuracy: 0.4034\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.9492 - accuracy: 0.3909\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.8414 - accuracy: 0.4309\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.8269 - accuracy: 0.4425\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.7603 - accuracy: 0.4556\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.7217 - accuracy: 0.4709\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.6715 - accuracy: 0.4800\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.6229 - accuracy: 0.4975\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.5738 - accuracy: 0.5075\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 1.5515 - accuracy: 0.5306\n",
      "   First_task training accuracy: 0.5306\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 4s 65ms/step - loss: 9.9637 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 8.3072 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 6.9339 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 2s 63ms/step - loss: 5.8699 - accuracy: 0.0063\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 2s 66ms/step - loss: 5.1465 - accuracy: 0.0200\n",
      "   1_task training accuracy: 0.0200\n",
      "Task 1 accuracy after training on Task ~0: 0.0712\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 4s 70ms/step - loss: 8.4687 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 2s 66ms/step - loss: 7.3023 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 6.4578 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 5.8962 - accuracy: 0.0012\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 2s 67ms/step - loss: 5.1050 - accuracy: 0.0125\n",
      "   2_task training accuracy: 0.0125\n",
      "Task 2 accuracy after training on Task ~1: 0.0660\n"
     ]
    }
   ],
   "source": [
    "# 모델 빌드 \n",
    "model = DFNet.build(input_shape=(10000, 1), classes=MAX_LABEL)\n",
    "# 옵티마이저 설정 \n",
    "OPTIMIZER = Adamax(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "train_loop(model, OPTIMIZER, data, test_size=0.2, first_task = 19, inc_task = 5, first_epochs = 30, inc_epochs = 5, lamb=1, num_sample=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naivis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
